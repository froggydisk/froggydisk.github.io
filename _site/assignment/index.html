<!DOCTYPE html> <html lang="en-US"> <head prefix="og: http://ogp.me/ns#"> <meta charset="UTF-8" /> <meta http-equiv="X-UA-Compatible" content="ie=edge" /> <meta name="viewport" content="width=device-width, initial-scale=1.0" /> <meta name="mobile-web-app-capable" content="yes" /> <meta name="apple-mobile-web-app-capable" content="yes" /> <meta name="application-name" content="froggydisk" /> <meta name="apple-mobile-web-app-status-bar-style" content="#fff" /> <meta name="apple-mobile-web-app-title" content="froggydisk" /> <title> CAM: Class Activation Map Implementation - froggydisk </title> <link rel="alternate" href="http://localhost:4000/assignment/" hreflang="en-US" /> <link rel="canonical" href="http://localhost:4000/assignment/" /> <meta name="description" content="This is for frogs in the world." /> <meta name="referrer" content="no-referrer-when-downgrade" /> <meta property="fb:app_id" content="" /> <meta property="og:site_name" content="CAM: Class Activation Map Implementation | ./workspace" /> <meta property="og:title" content="CAM: Class Activation Map Implementation | ./workspace" /> <meta property="og:type" content="website" /> <meta property="og:url" content="http://localhost:4000/assignment/" /> <meta property="og:description" content="This is for frogs in the world." /> <meta property="og:image" content="https://github.com/froggydisk/froggydisk.github.io/blob/master/assets/img/profile.png?raw=true" /> <meta property="og:image:width" content="640" /> <meta property="og:image:height" content="640" /> <meta name="twitter:card" content="summary" /> <meta name="twitter:title" content="CAM: Class Activation Map Implementation | twitter_username" /> <meta name="twitter:url" content="http://localhost:4000/assignment/" /> <meta name="twitter:site" content="@twitter_username" /> <meta name="twitter:creator" content="@twitter_username" /> <meta name="twitter:description" content="This is for frogs in the world." /> <meta name="twitter:image" content="https://github.com/froggydisk/froggydisk.github.io/blob/master/assets/img/profile.png?raw=true" /> <link type="application/atom+xml" rel="alternate" href="http://localhost:4000/feed.xml" title="froggydisk" /> <link rel="apple-touch-icon" sizes="180x180" href="/assets/favicons/apple-touch-icon.png" /> <link rel="icon" type="image/png" sizes="32x32" href="/assets/favicons/favicon-32x32.png" /> <link rel="icon" type="image/png" sizes="16x16" href="/assets/favicons/favicon-16x16.png" /> <link rel="manifest" href="/assets/favicons/site.webmanifest" /> <link rel="mask-icon" href="/assets/favicons/safari-pinned-tab.svg" color="#5bbad5" /> <meta name="apple-mobile-web-app-title" content="Jekyll Klise" /> <meta name="application-name" content="Jekyll Klise" /> <meta name="msapplication-TileColor" content="#da532c" /> <meta name="theme-color" content="#2c2c2c" /> <link rel="stylesheet" href="/assets/css/style.css" /> <script type="text/x-mathjax-config"> MathJax.Hub.Config({ TeX: { equationNumbers: { autoNumber: "AMS" } }, tex2jax: { inlineMath: [ ['$', '$'] ], displayMath: [ ['$$', '$$'] ], processEscapes: true, } }); MathJax.Hub.Register.MessageHook("Math Processing Error",function (message) { alert("Math Processing Error: "+message[1]); }); MathJax.Hub.Register.MessageHook("TeX Jax - parse error",function (message) { alert("Math Processing Error: "+message[1]); }); </script> <script type="text/javascript" async src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-MML-AM_CHTML"> </script> <script async src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js?client=ca-pub-4715878791193779" crossorigin="anonymous" ></script> <script type="importmap"> { "imports": { "three": "https://cdn.jsdelivr.net/npm/three@0.166.1/build/three.module.js", "three/addons/": "https://cdn.jsdelivr.net/npm/three@0.166.1/examples/jsm/" } } </script> </head> <body data-theme="dark" class="notransition"> <script> const body = document.body; const data = body.getAttribute("data-theme"); const initTheme = (state) => { if (state === "dark") { body.setAttribute("data-theme", "dark"); } else if (state === "light") { body.removeAttribute("data-theme"); } else { localStorage.setItem("theme", data); } }; initTheme(localStorage.getItem("theme")); setTimeout(() => body.classList.remove("notransition"), 75); </script> <div class="navbar" role="navigation"> <nav class="menu"> <input type="checkbox" id="menu-trigger" class="menu-trigger" /> <label for="menu-trigger"> <span class="menu-icon"> <svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 512 512" > <path d="M64,384H448V341.33H64Zm0-106.67H448V234.67H64ZM64,128v42.67H448V128Z" /> </svg> </span> </label> <a id="mode"> <svg class="mode-sunny" xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 512 512" > <title>LIGHT</title> <line x1="256" y1="48" x2="256" y2="96" style="stroke-linecap:round;stroke-miterlimit:10;stroke-width:32px" /> <line x1="256" y1="416" x2="256" y2="464" style="stroke-linecap:round;stroke-miterlimit:10;stroke-width:32px" /> <line x1="403.08" y1="108.92" x2="369.14" y2="142.86" style="stroke-linecap:round;stroke-miterlimit:10;stroke-width:32px" /> <line x1="142.86" y1="369.14" x2="108.92" y2="403.08" style="stroke-linecap:round;stroke-miterlimit:10;stroke-width:32px" /> <line x1="464" y1="256" x2="416" y2="256" style="stroke-linecap:round;stroke-miterlimit:10;stroke-width:32px" /> <line x1="96" y1="256" x2="48" y2="256" style="stroke-linecap:round;stroke-miterlimit:10;stroke-width:32px" /> <line x1="403.08" y1="403.08" x2="369.14" y2="369.14" style="stroke-linecap:round;stroke-miterlimit:10;stroke-width:32px" /> <line x1="142.86" y1="142.86" x2="108.92" y2="108.92" style="stroke-linecap:round;stroke-miterlimit:10;stroke-width:32px" /> <circle cx="256" cy="256" r="80" style="stroke-linecap:round;stroke-miterlimit:10;stroke-width:32px" /> </svg> <svg class="mode-moon" xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 512 512" > <title>DARK</title> <line x1="256" y1="48" x2="256" y2="96" style="stroke-linecap:round;stroke-miterlimit:10;stroke-width:32px" /> <line x1="256" y1="416" x2="256" y2="464" style="stroke-linecap:round;stroke-miterlimit:10;stroke-width:32px" /> <line x1="403.08" y1="108.92" x2="369.14" y2="142.86" style="stroke-linecap:round;stroke-miterlimit:10;stroke-width:32px" /> <line x1="142.86" y1="369.14" x2="108.92" y2="403.08" style="stroke-linecap:round;stroke-miterlimit:10;stroke-width:32px" /> <line x1="464" y1="256" x2="416" y2="256" style="stroke-linecap:round;stroke-miterlimit:10;stroke-width:32px" /> <line x1="96" y1="256" x2="48" y2="256" style="stroke-linecap:round;stroke-miterlimit:10;stroke-width:32px" /> <line x1="403.08" y1="403.08" x2="369.14" y2="369.14" style="stroke-linecap:round;stroke-miterlimit:10;stroke-width:32px" /> <line x1="142.86" y1="142.86" x2="108.92" y2="108.92" style="stroke-linecap:round;stroke-miterlimit:10;stroke-width:32px" /> <circle cx="256" cy="256" r="80" style="stroke-linecap:round;stroke-miterlimit:10;stroke-width:32px" /> </svg> </a> <div class="trigger"> <div class="trigger-container"><a class="menu-link" href="/">home</a><a class="menu-link" href="/archive/">archive</a><a class="menu-link" href="https://velog.io/@frog" target="_blank" rel="noopener" >velog</a ><a class="menu-link rss" href="https://github.com/froggydisk"> <svg xmlns="http://www.w3.org/2000/svg" width="17" height="17" viewBox="0 0 172 172" style=" fill:#000000;" > <title>github</title> <g fill="none" fill-rule="nonzero" stroke="none" stroke-width="1" stroke-linecap="butt" stroke-linejoin="miter" stroke-miterlimit="10" stroke-dasharray="" stroke-dashoffset="0" font-family="none" font-weight="none" font-size="none" text-anchor="none" style="mix-blend-mode: normal"> <path d="M0,172v-172h172v172z" fill="none"></path> <path d="M86,172c-47.49649,0 -86,-38.50351 -86,-86v0c0,-47.49649 38.50351,-86 86,-86v0c47.49649,0 86,38.50351 86,86v0c0,47.49649 -38.50351,86 -86,86z" fill="#ffffff"></path> <g> <path d="M86,53.86944c-22.66458,0 -57.69167,5.49444 -57.69167,43.95556c0,21.64536 19.23056,41.20833 43.95556,41.20833c2.74722,0 10.98889,0 13.73611,0c0,-22.66458 0,-85.16389 0,-85.16389z" fill="#000000"></path> <path d="M86,53.86944c22.66458,0 57.69167,5.49444 57.69167,43.95556c0,21.64536 -19.23056,41.20833 -43.95556,41.20833c-2.74722,0 -10.98889,0 -13.73611,0c0,-22.66458 0,-85.16389 0,-85.16389z" fill="#000000"></path> <path d="M42.04444,40.13333c-4.12083,8.24167 -5.49444,21.97778 -5.49444,33.65347c6.86806,-1.37361 35.71389,-17.17014 35.71389,-17.17014c-2.74722,-5.49444 -19.23056,-15.10972 -30.21944,-16.48333z" fill="#000000"></path> <path d="M129.95556,40.13333c4.12083,8.24167 5.49444,21.97778 5.49444,33.65347c-6.86806,-1.37361 -35.71389,-17.17014 -35.71389,-17.17014c2.74722,-5.49444 19.23056,-15.10972 30.21944,-16.48333z" fill="#000000"></path> <path d="M129.95556,106.06667c0,-12.13723 -9.84055,-21.97778 -21.97778,-21.97778c-10.98889,0 -15.10972,2.74722 -21.97778,2.74722c-6.86806,0 -10.98889,-2.74722 -21.97778,-2.74722c-12.13723,0 -21.97778,9.84055 -21.97778,21.97778c0,6.93948 3.22524,13.12073 8.24991,17.14816l-0.00824,0.02198c0.22527,0.17857 0.47527,0.33241 0.70878,0.49999c0.34066,0.25274 0.68131,0.50274 1.03845,0.73626c0.18132,0.11538 0.36813,0.22253 0.55219,0.33516c0.42032,0.25824 0.8434,0.50824 1.28021,0.739c9.74989,5.23346 23.62611,5.24445 32.13426,5.24445c8.50815,0 22.38437,-0.01099 32.13151,-5.24445c0.43956,-0.22802 0.85988,-0.48076 1.28021,-0.739c0.18406,-0.11264 0.37362,-0.21978 0.55219,-0.33516c0.35714,-0.23351 0.69779,-0.48351 1.03845,-0.73626c0.23351,-0.17033 0.48076,-0.32417 0.70878,-0.49999l-0.00824,-0.02198c5.02742,-4.02743 8.25266,-10.20868 8.25266,-17.14816z" fill="#ffffff"></path> <ellipse cx="23.80435" cy="39.6087" transform="scale(2.74722,2.74722)" rx="2.5" ry="4" fill="#000000"></ellipse> <ellipse cx="38.80435" cy="39.6087" transform="scale(2.74722,2.74722)" rx="2.5" ry="4" fill="#000000"></ellipse> <path d="M36.58846,71.27584c-0.01374,0.8379 -0.03846,1.68954 -0.03846,2.51096l35.71389,-17.17014c-0.28296,-0.56318 -0.74175,-1.17856 -1.29119,-1.81866c-12.71689,1.67581 -26.07389,6.06861 -34.38423,16.47784z" fill="#000000"></path> <path d="M135.41154,71.27584c-8.31309,-10.40922 -21.66734,-14.80203 -34.38423,-16.47509c-0.54944,0.63736 -1.00823,1.25273 -1.29119,1.81591l35.71389,17.17014c0,-0.82142 -0.02472,-1.67306 -0.03846,-2.51096z" fill="#000000"></path> </g> <path d="" fill="none"></path> </g> </svg> </a> </div> </div> </nav> </div> <div class="wrapper post"> <main class="page-content" aria-label="Content"> <article itemscope itemtype="https://schema.org/BlogPosting"> <header class="header"> <div class="tags"> <span itemprop="keywords"> <a class="tag" href="/tags/#pytorch">PYTORCH</a> </span> </div> <h1 class="header-title" itemprop="headline">CAM: Class Activation Map Implementation</h1> <div class="post-meta"> <time datetime="2020-08-04T00:00:00+07:00" itemprop="datePublished"> Aug 04, 2020 </time> <span itemprop="author" itemscope itemtype="https://schema.org/Person"> <span itemprop="name">./workspace</span> </span> <time hidden datetime="" itemprop="dateModified"> Aug 04, 2020 </time> <span hidden itemprop="publisher" itemtype="Person">./workspace</span> <span hidden itemprop="image"></span> <span hidden itemprop="mainEntityOfPage">Learning Deep Features for Discriminative Localization</span> </div> </header> <div class="page-content" itemprop="articleBody"> <p><br /></p> <p align="justify"> Today, I'm going to talk about a paper, 'Learning Deep Features for Discriminative Localization' (<a href="https://arxiv.org/pdf/1512.04150.pdf">Zhou+ CVPR16</a>). I will review this paper lightly first and try to implement the main functions of CAM with Pytorch, which are introduced in the paper. </p> <h2 id="background"> <a href="#background" class="anchor-head"></a> Background </h2> <p align="justify"> Over a long history of CNN, people made a great effort to increase the accuracy of the trained model. Since 2012, deep learning techniques have developed overwhelmingly and have now surpassed human beings in terms of object recognition. However, what they had known was that filters could find the edge of an object at the shallow layers and capture the high-dimensional features as the network goes deeper. The important thing is, they didn't know why the machine made such a judgment when it was asked to guess what the object was. That is to say, there was not any method that could explain the process of the machine's thinking. <br /><br /> <em>CAM</em>, which was developed by Bolei Zhou+ in 2016, is the very way to solve this problem. Using CAM, we can interpret an image from the machine's point of view and explain which parts of the image are utilized to make a decision. </p> <h2 id="introduction"> <a href="#introduction" class="anchor-head"></a> Introduction </h2> <p align="justify"> In this paper, the author showed that trained CNN model is successfully able to localize the discriminative regions of an object for classification despite no data on the location of the object was provided. </p> <p align="justify"> Generally, a CNN model has a Fully-Connected layer(FC) at the last part of the network, which is used to generate the final output. To use the FC layer, however, the features need to be flattened while losing the spatial information that convolution layers accumulated through the previous parts of the network. This is the main reason that makes the model lose the ability to localize objects. <br /> Recently, popular CNN models such as NIN(Network in Network) and GoogLeNet have been proposed to avoid FC layer not only to keep the localization ability which is mentioned above, but also to minimize the number of parameters while maintaining high performance. <br /> In order to achive this, the author just applied GAP(global average pooling) to the model. The use of GAP prevented overfitting during training and encouraged the network to identify the complete extent of the object at the same time. After the application of GAP, trained CNN model actually gets to build a generic localizable deep representation that exposes the implicit attention of CNNs on an image. </p> <p align="justify"> What we have to look at is that while GAP is not a novel techinique at all, which is even simple technique with little computational cost, the unique observation that it can be applied for accurate discriminative localizations offered a new paradigm in ML model analysis. <br /> This approach should be the core contribution of this paper and it provides us with another glimpse into the soul of CNNs. </p> <h2 id="class-activation-mapcam"> <a href="#class-activation-mapcam" class="anchor-head"></a> Class Activation Map(CAM) </h2> <p align="justify"> CAM actually works at the end of the network, just before the final output layer(softmax in the case of categorization). At this point, GAP is applied to the convolutional feature maps and the features after the GAP layer finally pass through the last FC layer. (This network uses only one FC layer) And then, CAM identifies the importance of the image regions by projecting back the weights of the output layer onto the convolutional feature maps. </p> <p><strong>To explain the concept with equations,</strong><br /> Let $f_k(x,y)$ represent the activation of unit k in the last convolutional layer at spatial location (x,y). Then, for unit k, the result of performing GAP is expressed as $F^k$ and it equals to $\sum_{x, y}f_k(x,y)$. Thus for a given class c, the input to the softmax, $S_c$ is $\sum_k w_{k}^{c}F_k$, where $w_{k}^{c}$ is the weight corresponding to class c for unit k. Essentially, $w_{k}^{c}$ indicates the importnace of $F_k$ for class c. To sum up, it becomes like this.</p> <p>$\begin{matrix} S_c &amp;=&amp; \sum_k w_k^c F_k <br /> &amp;=&amp; \sum_k w_c^k \sum_{x, y}f_k(x,y) <br /> &amp;=&amp; \sum_{x, y} \sum_k w_k^c f_k(x,y) \end{matrix}$</p> <p>If we define $M_c$ as the CAM for class c, where each spatial element is given by $M_c(x, y)$ = $\sum_k w_k^c f_k(x, y)$, we can find $S_c = \sum_{x, y} M_c(x, y)$. Hence, $M_c(x,y)$ directly indicates the importance of the activation at spatial grid (x,y) leading to the classification of an image to class c.</p> <p align="justify"> Therefore, the class activation map is simply a weighted linear sum of the presence of visual patterns at different spatial location. By simply upsampling the class activation map to the size of the input image, the regions of the image that are most relevant to the particular category can be identified. </p> <p><img src="https://github.com/froggydisk/froggydisk.github.io/blob/master/assets/img/CAM%20structure.png?raw=true" alt="structure" class="align-center" /></p> <h2 id="implementation"> <a href="#implementation" class="anchor-head"></a> Implementation </h2> <p align="justify"> I tried to implement the main functions of the original project and visually check their results. You can refer to the code <a href="https://github.com/froggydisk/CAM">here(CAM Implementation)</a>. I used Pytorch and you can simply run the code on GoogleColab. </p> <p align="justify"> I made a very simple network first and used CIFAR10 for training. After the training is finished, the model becomes able to classify an object and we can find the class with the highest probability. Then, we can draw a heatmap by multiplying the corresponding weights with each feature maps that came out from the last convolutional layer. </p> <p>The code below shows the main parts of the CAM function.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">feature_collection</span> <span class="o">=</span> <span class="p">[]</span> 
<span class="c1"># get features from the input
</span><span class="k">def</span> <span class="nf">get_feature</span><span class="p">(</span><span class="nb">input</span><span class="p">):</span>                                         
  <span class="n">_</span><span class="p">,</span> <span class="n">feature</span> <span class="o">=</span> <span class="nf">net</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span>
  <span class="n">feature_collection</span><span class="p">.</span><span class="nf">append</span><span class="p">(</span><span class="n">feature</span><span class="p">.</span><span class="nf">cpu</span><span class="p">().</span><span class="n">data</span><span class="p">.</span><span class="nf">numpy</span><span class="p">())</span>

<span class="n">params</span> <span class="o">=</span> <span class="nf">list</span><span class="p">(</span><span class="n">net</span><span class="p">.</span><span class="nf">parameters</span><span class="p">())</span>
<span class="c1"># get weights from the final layer
</span><span class="n">weight_for_softmax</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">squeeze</span><span class="p">(</span><span class="n">params</span><span class="p">[</span><span class="o">-</span><span class="mi">2</span><span class="p">].</span><span class="nf">cpu</span><span class="p">().</span><span class="n">data</span><span class="p">.</span><span class="nf">numpy</span><span class="p">())</span>

<span class="c1"># draw a heatmap
</span><span class="k">def</span> <span class="nf">Do_CAM</span><span class="p">(</span><span class="n">feature</span><span class="p">,</span> <span class="n">weigth_for_softmax</span><span class="p">,</span> <span class="n">class_id</span><span class="p">):</span> 
  <span class="n">upsample_size</span> <span class="o">=</span> <span class="p">(</span><span class="n">img_size</span><span class="p">,</span> <span class="n">img_size</span><span class="p">)</span>
  <span class="n">_</span><span class="p">,</span> <span class="n">c</span><span class="p">,</span> <span class="n">h</span><span class="p">,</span> <span class="n">w</span> <span class="o">=</span> <span class="n">feature</span><span class="p">.</span><span class="n">shape</span>
  <span class="c1"># (weights) x (feature maps)
</span>  <span class="n">cam</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">dot</span><span class="p">(</span><span class="n">weight_for_softmax</span><span class="p">[</span><span class="n">class_id</span><span class="p">],</span><span class="n">feature</span><span class="p">.</span><span class="nf">reshape</span><span class="p">(</span><span class="n">c</span><span class="p">,</span> <span class="n">h</span><span class="o">*</span><span class="n">w</span><span class="p">))</span>  
  <span class="n">cam</span> <span class="o">=</span> <span class="n">cam</span><span class="p">.</span><span class="nf">reshape</span><span class="p">(</span><span class="n">h</span><span class="p">,</span> <span class="n">w</span><span class="p">)</span>
  <span class="n">cam</span> <span class="o">=</span> <span class="p">(</span><span class="n">cam</span> <span class="o">-</span> <span class="n">np</span><span class="p">.</span><span class="nf">min</span><span class="p">(</span><span class="n">cam</span><span class="p">))</span> 
  <span class="n">cam</span> <span class="o">=</span> <span class="n">cam</span> <span class="o">/</span> <span class="n">np</span><span class="p">.</span><span class="nf">max</span><span class="p">(</span><span class="n">cam</span><span class="p">)</span>
  <span class="n">cam</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">uint8</span><span class="p">(</span><span class="mi">255</span> <span class="o">*</span> <span class="n">cam</span><span class="p">)</span>
  <span class="n">cam</span> <span class="o">=</span> <span class="n">cv2</span><span class="p">.</span><span class="nf">resize</span><span class="p">(</span><span class="n">cam</span><span class="p">,</span> <span class="n">upsample_size</span><span class="p">)</span>
  <span class="k">return</span> <span class="n">cam</span>
</code></pre></div></div> <p>By implementing this code, you can get a heatmap of the image you want to see.</p> <p><strong>Settings</strong></p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">dataset</span> <span class="o">=</span> <span class="sh">'</span><span class="s">CIFAR10</span><span class="sh">'</span>
<span class="n">img_size</span> <span class="o">=</span> <span class="mi">32</span>
<span class="n">batch_size</span> <span class="o">=</span> <span class="mi">128</span>
<span class="n">epoch</span> <span class="o">=</span> <span class="mi">5</span>
<span class="n">learning_rate</span> <span class="o">=</span> <span class="mf">0.001</span>
</code></pre></div></div> <p>These are the settings for my experiment. The number of epochs can vary depending on the condition.</p> <p>Here are some results.</p> <p><img src="https://github.com/froggydisk/froggydisk.github.io/blob/master/assets/img/bird1.jpg?raw=true" alt="test1" /> <img src="https://github.com/froggydisk/froggydisk.github.io/blob/master/assets/img/bird5.png?raw=true" alt="cam1" /><br /> <img src="https://github.com/froggydisk/froggydisk.github.io/blob/master/assets/img/horse1.jpg?raw=true" alt="test2" /> <img src="https://github.com/froggydisk/froggydisk.github.io/blob/master/assets/img/horse5-1.png?raw=true" alt="cam2" /><br /> <img src="https://github.com/froggydisk/froggydisk.github.io/blob/master/assets/img/ship1.jpeg?raw=true" alt="test3" /> <img src="https://github.com/froggydisk/froggydisk.github.io/blob/master/assets/img/ship40.png?raw=true" alt="cam3" /></p> <p>The results show that class activation maps are highlighting the discriminative object parts detected by the CNN.</p> <p>While implementing, it would be a bit hard to see the results because the size of CIFAR10 images is 32x32.<br /> I would recommend you to train the model with another dataset if possible, or just take a pretrained model like GoogLeNet for better results. If you use the pretrained model, you don’t need to train it anymore. You can use it as it is.</p> <p>And this is the results of comparison between 5-epoch-experiment and 50-epoch-experiment. I captured the model every 10 epochs during training and observed how the results change.</p> <p>         5 Epoch                 50 Epoch<br /> <img src="https://github.com/froggydisk/froggydisk.github.io/blob/master/assets/img/dog5-1.png?raw=true" alt="epoch5-1" title="5 epochs" /> <img src="https://github.com/froggydisk/froggydisk.github.io/blob/master/assets/img/dog50-1.png?raw=true" alt="epoch50-1" title="50 epochs" /><br /> <img src="https://github.com/froggydisk/froggydisk.github.io/blob/master/assets/img/dog5-2.png?raw=true" alt="epoch5-2" title="5 epochs" /> <img src="https://github.com/froggydisk/froggydisk.github.io/blob/master/assets/img/dog50-2.png?raw=true" alt="epoch50-2" title="50 epochs" /></p> <p>As expected, you can find the model concentrates on the details of the image as the number of epochs increases. Consequently, we can say global average pooling CNNs can perform accurate object localization.</p> <h2 id="reference"> <a href="#reference" class="anchor-head"></a> Reference </h2> <p><a href="https://kangbk0120.github.io/articles/2018-02/cam">https://kangbk0120.github.io/articles/2018-02/cam</a></p> </div> </article> <!-- unnecessary file, however you can still use for comment section, e.g disqus --> <script src="https://giscus.app/client.js" data-repo="froggydisk/comment-manager" data-repo-id="R_kgDOG7o-zQ" data-category="Announcements" data-category-id="DIC_kwDOG7o-zc4CN6cA" data-mapping="pathname" data-reactions-enabled="0" data-emit-metadata="0" data-input-position="bottom" data-theme="transparent_dark" data-lang="ko" crossorigin="anonymous" async> </script> </main> <nav class="post-nav"> <a class="post-nav-item post-nav-prev" href="/second-post/" > <div class="nav-arrow">Previous</div> <span class="post-title">Top1 Accuracy와 Top5 Accuracy 이해하기</span> </a> <a class="post-nav-item post-nav-next" href="/third-post/"> <div class="nav-arrow">Next</div> <span class="post-title">모바일에서 hover 효과 없애기</span> </a> </nav> <!DOCTYPE html> <html> <head> <script src="https://kit.fontawesome.com/296a556139.js" crossorigin="anonymous"></script> </head> <body> <div class="support-link-btn"> <!-- <button class="w-btn w-btn-yellow" onclick="location.href='https://qr.kakaopay.com/Ej7ycru63'"> <span>Give me a kakao</span> <i class="fa-solid fa-heart fa-beat" style="--fa-animation-duration: 1s;"></i> </button> --> <button class="w-btn w-btn-gra1 w-btn-gra-anim" onclick="location.href='https://buymeacoffee.com/froggydisk'"> <span>Buy me a coffee</span> <i class="fa-solid fa-mug-hot"></i> </button> </div> </body> </html> <style type="text/css"> @import url("https://fonts.googleapis.com/css?family=Poppins:200,300,400,500,600,700,800,900&display=swap"); .support-link-btn { text-align: center; margin-top: 3em; } .w-btn { font-size: 0.8em; position: relative; border: none; display: inline-block; padding: 15px 30px; border-radius: 15px; font-family: "paybooc-Light", sans-serif; box-shadow: 0 15px 35px rgba(0, 0, 0, 0.2); text-decoration: none; font-weight: 600; transition: 0.25s; } .w-btn:hover { transform: scale(1.1); cursor: pointer; } .w-btn:active { transform: scale(1.1); } @keyframes ring { 0% { width: 30px; height: 30px; } 100% { width: 300px; height: 300px; border-color: transparent; } } .w-btn:hover::after { content: ""; border-radius: 100%; border: 6px solid #9dc8c8; position: absolute; z-index: -1; top: 50%; left: 50%; transform: translate(-50%, -50%); animation: ring 1.5s infinite; } .w-btn-yellow { background-color: #fce205; color: #3B240B; } .w-btn-gra1 { background: linear-gradient(-45deg, #ee7752, #e73c7e, #23a6d5, #23d5ab); color: white; } .w-btn-gra-anim { background-size: 400% 400%; animation: gradient 7s ease infinite; } @keyframes gradient { 0% { background-position: 0% 50%; } 50% { background-position: 100% 50%; } 100% { background-position: 0% 50%; } } </style> <footer class="footer"> <div style="display: flex; align-items: center; justify-content: center"> <a class="letter-image" href="/resume/"> <div class="animated-mail"> <div class="back-fold"></div> <div class="letter"> <div class="letter-title"></div> <div class="letter-context"></div> <div class="letter-stamp"> <div class="letter-stamp-inner"></div> </div> </div> <div class="top-fold"></div> <div class="body"></div> <div class="left-fold"></div> </div> <span style="margin-left: 25px">resume</span> </a> <!-- <a class="footer_item" href="/resume/">resume</a> --> <span class="footer_item">&copy; 2024</span> </div> <small class="footer_copyright"> <!-- Klisé Theme: https://github.com/piharpi/jekyll-klise --> <a href="https://github.com/piharpi/jekyll-klise" target="_blank" rel="noreferrer noopener" >klisé</a > theme on <a href="https://jekyllrb.com" target="_blank" rel="noreferrer noopener" >jekyll</a > </small> </footer> <script src="/assets/js/main.js" defer="defer"></script> </div> </body> </html>
